{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Decision Support System.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN14d6LJF2vT+uZfgwtwMNM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"dnBux_r92Tq6"},"source":["#Decision Support System\r\n","This notebook enables researchers to retrieve citations through accessing multiple API's at once.\r\n","\r\n","\r\n","**Requirements:**\r\n","\r\n","\r\n","*   API keys for Pubmed and Elsevier API\r\n","*   For Elsevier's API, you must have a connection via an Academic IP address (Can be through VPN)\r\n","\r\n","**We have several options:**\r\n","1.   Automated data retrieval, manual labeling and automated study selection\r\n","2.   Manual labeling and automated study selection \r\n","3.   Automated study selection\r\n","\r\n","To execute option 2 and 3, please upload your database results to the system, and run the corresponding cells.\r\n","\r\n","\r\n","\r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"gsfqWSXt5QVR"},"source":["#@title Run this cell to install to import all required libraries { display-mode: \"form\" }\r\n","!pip install elsapy\r\n","!pip install biopython\r\n","from Bio import Entrez\r\n","from elsapy.elsclient import ElsClient\r\n","from elsapy.elsprofile import ElsAuthor, ElsAffil\r\n","from elsapy.elsdoc import FullDoc, AbsDoc\r\n","from elsapy.elssearch import ElsSearch\r\n","import pandas as pd\r\n","import ipywidgets as widgets\r\n","import csv \r\n","from Bio import Medline\r\n","import requests\r\n","import re\r\n","from requests.adapters import HTTPAdapter\r\n","from requests.packages.urllib3.util.retry import Retry\r\n","from IPython.display import HTML, display, clear_output\r\n","import ipywidgets as widgets\r\n","from sklearn.model_selection import train_test_split\r\n","import numpy as np\r\n","\r\n","# Log in to google drive to access files\r\n","from google.colab import drive\r\n","drive.mount('/gdrive')\r\n","%cd /gdrive/'My Drive'/'Decision Support System'\r\n","\r\n","# Custom scripts\r\n","from metrics import WorkSavedOverSamplingAtRecall\r\n","from preprocessing import Preprocessor\r\n","from model import multichannel_cnn\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m6hw-bHk9nak"},"source":["#@title # Step 1 - Data Retrieval { display-mode: \"form\" }\r\n","#@markdown ---\r\n","\r\n","#@markdown ### General information\r\n","elsevier_api_key = '' #@param {type:'string'} \r\n","springer_api_key = '' #@param {type:'string'} \r\n","email = \"lorem@ipsum.org\" #@param {type:'string'}\r\n","#lorem@ipsum.org\r\n","Entrez.email = email\r\n","#@markdown ---\r\n","\r\n","#@markdown ### Databases\r\n","pubmed = True #@param {type:\"boolean\"}\r\n","sciencedirect = False #@param {type:\"boolean\"}\r\n","springer = True #@param {type:\"boolean\"}\r\n","\r\n","#@markdown ---\r\n","\r\n","#@markdown ### Search query\r\n","search_query = \"\\\"Systematic review\\\" AND automation\" #@param {type:'string'}\r\n","#(Automation OR Automate OR Automates OR Automating) AND (\\\"Systematic Literature Review\\\" OR \\\"Systematic Review\\\")\r\n","\r\n","#@markdown ### Search fields\r\n","## @markdown If you didn't choose the database to search, it does not matter what field you choose for that particular database\r\n","\r\n","# EXAMPLE: Here, we request all possible fields for the PubMed database.\r\n","# print possible_fields_df if you want to use another field that Title/Abstract\r\n","# Through Entrez.einfo, you can also find all other databases and their fields.\r\n","# record = Entrez.read(Entrez.einfo(db='pubmed'))\r\n","# possible_fields_df = pd.DataFrame(record['DbInfo']['FieldList'])\r\n","\r\n","pubmed_field = 'TIAB' #@param ['ALL', 'UID', 'FILT', 'TITL', 'WORD', 'MESH', 'MAJR', 'AUTH', 'JOUR', 'AFFL', 'ECNO', 'SUBS', 'PDAT', 'EDAT', 'VOL', 'PAGE', 'PTYP', 'LANG', 'ISS', 'SUBH', 'SI', 'MHDA', 'TIAB', 'OTRM', 'INVR', 'COLN', 'CNTY', 'PAPX', 'GRNT', 'MDAT', 'CDAT', 'PID', 'FAUT', 'FULL', 'FINV', 'TT', 'LAUT', 'PPDT', 'EPDT', 'LID', 'CRDT', 'BOOK', 'ED', 'ISBN', 'PUBN', 'AUCL', 'EID', 'DSO', 'AUID', 'PS', 'COIS'] {type:\"string\"}\r\n","springer_field = 'all' #@param ['all', 'title', 'orgname', 'journal', 'book', 'name'] {type:\"string\"}\r\n","\r\n","#@markdown ### Max returned citations per database\r\n","pubmed_maxret = 200 #@param {type:\"number\"}\r\n","springer_maxret = 300 #@param {type:\"number\"}\r\n","\r\n","#@markdown ### Timeframe\r\n","##@markdown Springer only searches on year, not exact date\r\n","start_date = '2000/01/01' #@param {type:\"date\"}\r\n","end_date = '2021/01/01' #@param {type:\"date\"}\r\n","\r\n","#@markdown ---\r\n","\r\n","#@markdown Press SHIFT+ENTER or click the play button to execute the query\r\n","\r\n","# The code below descibes the retrieval of documents from the API's\r\n","\r\n","def springer_query_tagging(query, tag):\r\n","    '''\r\n","    This function ensures that the search query for Springer is correctly tagged\r\n","    '''\r\n","    if tag == 'all':\r\n","        # query does not need to be tagged\r\n","        return query\r\n","\r\n","    # split the query, while keeping all characters\r\n","    query_lst = (re.split('(\\W)', query))\r\n","\r\n","    tag = tag + ':'\r\n","    boolean_operators = ['OR', 'AND', 'NOT']\r\n","    brackets = ['(', ')']    \r\n","    spaces = [' ', '']        \r\n","    iseven = False\r\n","\r\n","    tagged_query_lst = []\r\n","\r\n","    # go through every item in the query, if it is a term, add the tag before it\r\n","    for item in query_lst:\r\n","        if item.upper() not in boolean_operators:\r\n","            if item not in brackets and item not in spaces: \r\n","                if item is not '\"' and not iseven:\r\n","                    tagged_query_lst.append(tag+item)\r\n","                elif item is '\"' and not iseven:\r\n","                    tagged_query_lst.append(tag+item)\r\n","                    iseven = not iseven\r\n","                elif item is '\"' and iseven:\r\n","                    tagged_query_lst.append(item)\r\n","                    iseven = not iseven\r\n","                else:\r\n","                    tagged_query_lst.append(item)\r\n","            elif item in brackets:\r\n","                pass\r\n","            else:\r\n","                tagged_query_lst.append(item)\r\n","        else:\r\n","            tagged_query_lst.append(item)\r\n","    return ''.join(tagged_query_lst)\r\n","\r\n","def springer_search(q, api_key, start_date, end_date, retmax):\r\n","    # adopt a retry strategy for http requests\r\n","    retry_strategy = Retry(\r\n","                    total=3,\r\n","                    status_forcelist=[429, 500, 502, 503, 504],\r\n","                    method_whitelist=[\"HEAD\", \"GET\", \"OPTIONS\"]\r\n","                    )\r\n","    adapter = HTTPAdapter(max_retries=retry_strategy)\r\n","    http = requests.Session()\r\n","    http.mount(\"https://\", adapter)\r\n","    http.mount(\"http://\", adapter)\r\n","\r\n","    start_result_position=1\r\n","    sort = 'sequence'\r\n","    results = []\r\n","\r\n","    # Get the query results untill the maximum number of documents has been retrieved, or no documents are available anymore \r\n","    while True:\r\n","        x = http.get(f'http://api.springernature.com/meta/v2/json?q={q} sort:{sort}&api_key={api_key}&s={start_result_position}&p={retmax}&date-facet-mode=between&facet-start-year={start_date[:4]}&showAll=true&facet-end-year={end_date[:4]}')\r\n","        start_result_position += 100\r\n","        results.append(pd.DataFrame(x.json()['records']))\r\n","\r\n","        if int(x.json()['result'][0]['recordsDisplayed']) == 0 or start_result_position > retmax:\r\n","            return pd.concat(results), x.json()['query']\r\n","\r\n","    return pd.concat(results), x.json()['query']\r\n","\r\n","query_results = []\r\n","\r\n","if pubmed:\r\n","    print('--- Pubmed ---')\r\n","    # Find all article IDs containing search query, sorted by relevance\r\n","    handle = Entrez.esearch(db=\"pubmed\", retmax=pubmed_maxret, term=search_query, sorted='relevance', idtype=\"acc\", field=pubmed_field, mindate=start_date, maxdate=end_date)\r\n","    record = Entrez.read(handle)\r\n","    handle.close()\r\n","\r\n","    # Retrieve all article data by ID\r\n","    idlist = record[\"IdList\"]\r\n","    handle = Entrez.efetch(db=\"pubmed\", id=idlist, rettype=\"medline\", retmode=\"text\")\r\n","    docs = []\r\n","\r\n","    # Parse data in medline format and save to file\r\n","    articles = Medline.parse(handle)\r\n","    for article in articles:\r\n","        docs.append(article)\r\n","    pubmed_df = pd.DataFrame(docs)\r\n","    pubmed_df = pubmed_df.rename(columns={'TI': 'title', 'AB':'abstract', 'AID': 'identifier', 'DP':'publicationDate'})\r\n","    pubmed_df['database'] = 'pubmed'\r\n","\r\n","    query_results.append(pubmed_df)\r\n","    print(\"The query interpreted by PubMed: {} \\nThis query resulted into {} records found.\".format(record['QueryTranslation'], record[\"Count\"]))\r\n","\r\n","\r\n","if springer:\r\n","    print('--- Springer ---')\r\n","    # Springer allows a maximum of 100 returns\r\n","    tagged_query = springer_query_tagging(search_query, springer_field)\r\n","    springer_df, springer_query = springer_search(tagged_query, springer_api_key, start_date, end_date, retmax=springer_maxret)\r\n","    springer_df['database'] = 'springer'\r\n","    query_results.append(springer_df)\r\n","    print(\"The tagged query: {} \\nThis query resulted into {} records found.\".format(springer_query, len(springer_df)))\r\n","\r\n","\r\n","if sciencedirect:\r\n","    print('--- ScienceDirect ---')\r\n","    ## Initialize client\r\n","    client = ElsClient(elsevier_api_key)\r\n","\r\n","    # If get_all = True, then all results will be retrieved, with a maximum of 5000, \r\n","    # otherwise, 20 results will be retrieved (1 API request)\r\n","    get_all = False\r\n","\r\n","    ## Initialize doc search object using ScienceDirect and execute search, \r\n","    #   retrieving all results\r\n","    doc_srch = ElsSearch(search_query, 'sciencedirect')\r\n","    doc_srch.execute(client, get_all=get_all)\r\n","    print(\"doc_srch has\", doc_srch.len_res(), \"results.\")\r\n","\r\n","    sciencedirect_df = doc_srch.results_df\r\n","    query_results.append(sciencedirect_df)\r\n","\r\n","if len(query_results) == 0:\r\n","    print('No database selected')\r\n","else:\r\n","    with pd.ExcelWriter('datasets/query_results.xlsx') as writer:  \r\n","        # Write each dataframe to a different worksheet.\r\n","        for result in query_results:\r\n","            result.to_excel(writer, sheet_name=result['database'].iloc[0])\r\n","\r\n","    print('Articles have been saved to datasets/query_results.xlsx')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"G3pvbnih2BY6"},"source":["#@title # Dataset splitting\r\n","#@markdown This cell should be executed before executing Step 2 or 3.\r\n","\r\n","#@markdown ---\r\n","#@markdown Put your data file name below.\r\n","slr_excel_file = 'query_results.xlsx' #@param {type:\"string\"}\r\n","#@markdown Make sure each database has its own results sheet, containing at least the following columns: 'title', 'publicationDate', 'identifier', 'database', and 'abstract'\r\n","\r\n","query_results_file = pd.ExcelFile('datasets/'+slr_excel_file)\r\n","sheets = query_results_file.sheet_names\r\n","Xtrains = []\r\n","Xtests = []\r\n","ytrains = []\r\n","ytests = []\r\n","for sheet in sheets:\r\n","    query_results = query_results_file.parse(sheet)\r\n","    if 'label' not in query_results.columns:\r\n","        query_results['label'] = np.nan\r\n","    \r\n","    # We want to split each databases' results independently, as they have been sorted based on relevance by the databases \r\n","    Xtrain, Xtest, ytrain, ytest = train_test_split(query_results[['title', 'publicationDate', 'identifier', 'abstract', 'database']], query_results['label'], test_size=0.5, shuffle=False)\r\n","    Xtrains.append(Xtrain)\r\n","    Xtests.append(Xtest)\r\n","    ytrains.append(ytrain)\r\n","    ytests.append(ytest)\r\n","\r\n","# We want to concatenate the search results from the databases, and alternating the rows to keep the most relevant articles on top for the training set.\r\n","Xtrain = pd.concat(Xtrains).sort_index(kind='merge')\r\n","ytrain = pd.concat(ytrains).sort_index(kind='merge')\r\n","Xtest = pd.concat(Xtests)\r\n","ytest = pd.concat(ytests)\r\n","train = pd.concat([Xtrain, ytrain], axis=1)\r\n","test = pd.concat([Xtest, ytest], axis=1)\r\n","train.to_excel('datasets/train.xlsx')\r\n","test.to_excel('datasets/test.xlsx')\r\n","\r\n","print('Train and test set have been split.')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KIuXwIrkJZM8","cellView":"form"},"source":["#@title # Step 2 - Manual labeling\r\n","#@markdown ---\r\n","\r\n","\r\n","#@markdown Run this cell to start manual classification of the train set\r\n","train_set = pd.read_excel('datasets/train.xlsx')\r\n","train_set = train_set[['title', 'publicationDate', 'identifier', 'abstract', 'database', 'label']]\r\n","\r\n","\r\n","already_classified = train_set[~train_set['label'].isna()]\r\n","train = train_set[train_set['label'].isna()].reset_index()\r\n","\r\n","\r\n","i = 0\r\n","\r\n","def on_button_clicked_include(b):\r\n","    global i\r\n","    train['label'][i] = True\r\n","    clear_output()\r\n","    if i < len(train):\r\n","        i += 1\r\n","        display(hbox)\r\n","        printable_abs = re.sub(\"(.{150} )\", \"\\\\1\\n\", train['abstract'][i], 0, re.DOTALL)\r\n","        print('\\nReview {} out of {}.\\n\\nTitle: {}\\nYear: {}\\nIdentifier: {}\\nDatabase:{}\\nAbstract: {}'.format(i+1, len(train)+1, train['title'][i], train['publicationDate'][i], train['identifier'][i], train['database'][i], printable_abs))\r\n","    else:\r\n","        print('All citations have been reviewed.')\r\n","        save_set = pd.concat([already_classified, train], ignore_index=True)\r\n","        save_set.to_excel('datasets/train.xlsx')\r\n","        print('Training set updated and saved.')\r\n","\r\n","def on_button_clicked_exclude(b):\r\n","    global i\r\n","    train['label'][i] = False\r\n","    clear_output()\r\n","    if i < len(train):\r\n","        i += 1\r\n","        display(hbox)\r\n","        printable_abs = re.sub(\"(.{150} )\", \"\\\\1\\n\", train['abstract'][i], 0, re.DOTALL)\r\n","        print('\\nReview {} out of {}.\\n\\nTitle: {}\\nYear: {}\\nIdentifier: {}\\nDatabase:{}\\nAbstract: {}'.format(i+1, len(train)+1, train['title'][i], train['publicationDate'][i], train['identifier'][i], train['database'][i], printable_abs))\r\n","    else:\r\n","        print('All citations have been reviewed.')\r\n","        save_set = pd.concat([already_classified, train], ignore_index=True)\r\n","        save_set.to_excel('datasets/train.xlsx')\r\n","        print('Training set updated and saved.')\r\n","\r\n","def on_button_clicked_save_and_exit(b):\r\n","    save_set = pd.concat([already_classified, train], ignore_index=True)\r\n","    save_set.to_excel('datasets/train.xlsx')\r\n","    print('Training set updated and saved.')\r\n","\r\n","if len(train) > 0:\r\n","    # Show buttons\r\n","    include = widgets.Button(description=\"Include\")\r\n","    exclude = widgets.Button(description=\"Exclude\")\r\n","    save_and_exit = widgets.Button(description=\"Save and Exit\")\r\n","    hbox = widgets.HBox([include, exclude, save_and_exit])\r\n","    display(hbox)\r\n","\r\n","    include.on_click(on_button_clicked_include)\r\n","    exclude.on_click(on_button_clicked_exclude)\r\n","    save_and_exit.on_click(on_button_clicked_save_and_exit)\r\n","\r\n","    printable_abs = re.sub(\"(.{150} )\", \"\\\\1\\n\", train['abstract'][i], 0, re.DOTALL)\r\n","    print('\\nReview {} out of {}.\\n\\nTitle: {}\\nYear: {}\\nIdentifier: {}\\nDatabase:{}\\nAbstract: {}'.format(i+1, len(train)+1, train['title'][i], train['publicationDate'][i], train['identifier'][i], train['database'][i], printable_abs))\r\n","\r\n","else:\r\n","    print('All articles have been labeled.')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"XxxSyNe2b0-C"},"source":["#@title # Step 3 - Citation Screening\r\n","train_path = 'datasets/train.xlsx'\r\n","test_path = 'datasets/test.xlsx'\r\n","embedding_path = 'embeddings/glove.6B.100d.txt'\r\n","buffer_size = 512\r\n","batch_size = 100\r\n","epochs=15\r\n","\r\n","preprocessor = Preprocessor()\r\n","train_ds, test_ds, steps_per_epoch = preprocessor.get_tf_datasets(train_path, test_path, embedding_path, batch_size, batch_size, show_imbalance=True)\r\n","\r\n","max_len = preprocessor.get_max_sequence_length()\r\n","model = multichannel_cnn()\r\n","\r\n","print('Training the model, this may take a few minutes depending on your dataset size')\r\n","history = model.fit(train_ds, steps_per_epoch=steps_per_epoch, epochs=epochs, verbose=0)\r\n","\r\n","# Make predictions and sort them in descending order to an excel file.\r\n","test = preprocessor._test\r\n","test['prediction'] = model.predict(test_ds) \r\n","test = test.sort_values('prediction', ascending=False)\r\n","test.to_excel('datasets/predictions.xlsx')\r\n","print('Predictions have been saved in datasets/predictions.xlsx')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"iBbyweFuks6k"},"source":["#@title # Model evaluation\r\n","#@markdown This cell can be executed to assess the performance of our model. This can only be performed when test dataset has also been labeled.\r\n","evaluation = model.evaluate(test_ds, verbose=0, return_dict=True)\r\n","\r\n","test = preprocessor._test\r\n","test = test.sort_values('prediction', ascending=True)\r\n","len_before = len(test)\r\n","test = test.loc[test[(test != 0).all(axis=1)].first_valid_index():]\r\n","len_after = len(test)\r\n","\r\n","print('Saved reading {} articles. A WSS@95% score of {:.2f}% was achieved.'.format(len_before-len_after, evaluation['wss_95']*100))"],"execution_count":null,"outputs":[]}]}